{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# **SHAKESPEARE'S SONNETS**\n",
    "\n",
    "## Problem\n",
    "Learn from [Shakespeare's sonnets]() (cleaned) to auto-generate poems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Input, Embedding, Flatten, Dense, GlobalAveragePooling1D, Bidirectional, LSTM, Conv1D, MaxPooling1D, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, TensorBoard\n",
    "from tensorflow.keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# set random seeds\n",
    "np.random.seed(0)\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "# show figures inline\n",
    "%matplotlib inline\n",
    "\n",
    "# ignore all warning (NOT recommended)\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Global inputs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "PAD_TYPE = 'pre'\n",
    "LOG_DIR = '03-logs'\n",
    "PROBLEM = 'poem'\n",
    "EMBED_DIMS = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "**Get the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['FROM fairest creatures we desire increase,',\n",
       " \"That thereby beauty's rose might never die,\",\n",
       " 'But as the riper should by time decease,',\n",
       " 'His tender heir might bear his memory:',\n",
       " 'But thou, contracted to thine own bright eyes,']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_url = 'https://storage.googleapis.com/laurencemoroney-blog.appspot.com/sonnets.txt'\n",
    "path_out = '../.tmp/sonnets.txt'\n",
    "\n",
    "# download the dataset\n",
    "if not os.path.exists(path_out):\n",
    "    os.system(f\"\"\"wget --no-check-certificate {data_url} -O {path_out}\"\"\")\n",
    "\n",
    "# read the text file\n",
    "with open(path_out, 'r') as f:\n",
    "    data = f.read().split('\\n')\n",
    "\n",
    "# check the data\n",
    "data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# convert all texts to lower case\n",
    "corpus = [line.lower() for line in data]\n",
    "\n",
    "# initiate a tokenizer\n",
    "tokenizer = Tokenizer(oov_token='<OOV>')\n",
    "# update the vocabulary on the texts\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "\n",
    "# total number of words\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "# create input sequences\n",
    "input_sequences = []\n",
    "max_len = 0\n",
    "for line in corpus:\n",
    "    # ignore empty lines\n",
    "    if line == '':\n",
    "        continue\n",
    "    # convert the text line to a list of tokens\n",
    "    token_line = tokenizer.texts_to_sequences([line])[0]\n",
    "    # create sequence of 1st 2, 3, ... words (a.k.a. n-gram sequence)\n",
    "    for i in range(1, len(token_line)):\n",
    "        n_gram_sequence = token_line[:i+1]\n",
    "        # append to the input sequences\n",
    "        input_sequences.append(n_gram_sequence)\n",
    "        # keep track of maximum length\n",
    "        max_len = max(max_len, len(n_gram_sequence))\n",
    "\n",
    "# padded sequences\n",
    "input_sequences = pad_sequences(input_sequences, padding=PAD_TYPE, maxlen=max_len)\n",
    "# convert to numpy array\n",
    "input_sequences = np.array(input_sequences)\n",
    "\n",
    "# the sequence up to 2nd to the last is predictor\n",
    "predictors = input_sequences[:, :-1]\n",
    "# the last one in the sequence is the label\n",
    "labels = input_sequences[:, -1]\n",
    "# convert labels to one-hot encodings\n",
    "labels = tf.keras.utils.to_categorical(labels, num_classes=total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data size: (15462, 11)\n",
      "predictor: [ 0  0  0  0  0  0  0  0  0 35], label: [0. 0. 0. ... 0. 0. 0.]\n",
      "predictor: [  0   0   0   0   0   0   0   0  35 418], label: [0. 0. 0. ... 0. 0. 0.]\n",
      "predictor: [  0   0   0   0   0   0   0  35 418 878], label: [0. 0. 0. ... 0. 0. 0.]\n",
      "predictor: [  0   0   0   0   0   0  35 418 878 167], label: [0. 0. 0. ... 0. 0. 0.]\n",
      "predictor: [  0   0   0   0   0  35 418 878 167 214], label: [0. 0. 0. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# check\n",
    "print(f'data size: {input_sequences.shape}')\n",
    "for i in range(5):\n",
    "    print(f'predictor: {predictors[i]}, label: {labels[i]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Pre-trained Embeddings\n",
    "\n",
    "**GloVe**\n",
    "([See more](https://nlp.stanford.edu/projects/glove/))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3212, 100)\n"
     ]
    }
   ],
   "source": [
    "data_url = 'https://storage.googleapis.com/laurencemoroney-blog.appspot.com/glove.6B.100d.txt'\n",
    "path_out = '../.tmp/glove.6B.100d.txt'\n",
    "save_mat = LOG_DIR + '/' + PROBLEM + '-glove_embedding.npy'\n",
    "\n",
    "# download the dataset\n",
    "if not os.path.exists(path_out):\n",
    "    os.system(f\"\"\"wget --no-check-certificate {data_url} -O {path_out}\"\"\")\n",
    "\n",
    "if os.path.exists(save_mat):\n",
    "    embeddings_matrix = np.load(save_mat)\n",
    "else:\n",
    "    embeddings_index = {}\n",
    "    with open(path_out, 'r', encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "\n",
    "    embeddings_matrix = np.zeros((total_words, EMBED_DIMS))\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embeddings_matrix[i] = embedding_vector\n",
    "\n",
    "    np.save(save_mat, embeddings_matrix)\n",
    "\n",
    "# pre-trained embeddings\n",
    "print(embeddings_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Model\n",
    "\n",
    "**Note**\n",
    "- We used a pre-trained embedding layers (kept the layer frozen).\n",
    "- Then, a bidirectional LSTM layer.\n",
    "- Before the output layer, we used a dense layer with kernel regulizer.\n",
    "- We used `'softmax'` activation function in output layer since it is a multi-class classification problem.\n",
    "- Since the multi-class labels are one-hot encoded, we used `'categorical_crossentropy'` as loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 10)]              0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 10, 100)           321200    \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 10, 300)           301200    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 10, 300)           0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               160400    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1606)              162206    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3212)              5161684   \n",
      "=================================================================\n",
      "Total params: 6,106,690\n",
      "Trainable params: 5,785,490\n",
      "Non-trainable params: 321,200\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# input layer\n",
    "input_tensor = Input(shape=max_len-1)  # -1 because we used one for label\n",
    "\n",
    "# pre-trained embedding layer\n",
    "x = Embedding(\n",
    "    input_dim=embeddings_matrix.shape[0],\n",
    "    output_dim=embeddings_matrix.shape[1],\n",
    "    input_length=max_len-1,  # -1 because we used one for label\n",
    "    weights=[embeddings_matrix],\n",
    "    trainable=False\n",
    ")(input_tensor)\n",
    "\n",
    "# hidden layers\n",
    "x = Bidirectional(LSTM(150, return_sequences=True))(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = LSTM(100)(x)\n",
    "x = Dense(total_words//2, activation='relu', kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "\n",
    "# output layer\n",
    "output_tensor = Dense(total_words, activation='softmax')(x)\n",
    "\n",
    "# model\n",
    "model = tf.keras.Model(input_tensor, output_tensor)\n",
    "\n",
    "# compile\n",
    "model.compile(\n",
    "    optimizer=Adam(),\n",
    "    loss='categorical_crossentropy'\n",
    ")\n",
    "\n",
    "# model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "\n",
    "**Callbacks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# reduce learning rate\n",
    "reduce_rl = ReduceLROnPlateau(\n",
    "    monitor='loss', mode='min', factor=0.1, patience=5, verbose=2\n",
    ")\n",
    "\n",
    "# stop training\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='loss',  mode='min', min_delta=0, patience=20, verbose=2, restore_best_weights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Start Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15462 samples\n",
      "Epoch 1/100\n",
      "15462/15462 [==============================] - 10s 624us/sample - loss: 6.9036 - los\n",
      "Epoch 2/100\n",
      "15462/15462 [==============================] - 5s 339us/sample - loss: 6.5539\n",
      "Epoch 3/100\n",
      "15462/15462 [==============================] - 5s 331us/sample - loss: 6.5006\n",
      "Epoch 4/100\n",
      "15462/15462 [==============================] - 5s 333us/sample - loss: 6.4584\n",
      "Epoch 5/100\n",
      "15462/15462 [==============================] - 5s 333us/sample - loss: 6.3882s - loss: 6 - ETA:\n",
      "Epoch 6/100\n",
      "15462/15462 [==============================] - 5s 336us/sample - loss: 6.3123s - loss: 6.314\n",
      "Epoch 7/100\n",
      "15462/15462 [==============================] - 5s 334us/sample - loss: 6.2389\n",
      "Epoch 8/100\n",
      "15462/15462 [==============================] - 5s 340us/sample - loss: 6.1585\n",
      "Epoch 9/100\n",
      "15462/15462 [==============================] - 5s 327us/sample - loss: 6.0835s - loss:  \n",
      "Epoch 10/100\n",
      "15462/15462 [==============================] - 5s 329us/sample - loss: 6.0050\n",
      "Epoch 11/100\n",
      "15462/15462 [==============================] - 5s 335us/sample - loss: 5.9245\n",
      "Epoch 12/100\n",
      "15462/15462 [==============================] - 5s 338us/sample - loss: 5.8403\n",
      "Epoch 13/100\n",
      "15462/15462 [==============================] - 5s 332us/sample - loss: 5.7532\n",
      "Epoch 14/100\n",
      "15462/15462 [==============================] - 5s 328us/sample - loss: 5.6619\n",
      "Epoch 15/100\n",
      "15462/15462 [==============================] - 5s 345us/sample - loss: 5.5702\n",
      "Epoch 16/100\n",
      "15462/15462 [==============================] - 5s 347us/sample - loss: 5.4833\n",
      "Epoch 17/100\n",
      "15462/15462 [==============================] - 5s 350us/sample - loss: 5.3948\n",
      "Epoch 18/100\n",
      "15462/15462 [==============================] - 5s 342us/sample - loss: 5.3137\n",
      "Epoch 19/100\n",
      "15462/15462 [==============================] - 5s 348us/sample - loss: 5.2278\n",
      "Epoch 20/100\n",
      "15462/15462 [==============================] - 5s 348us/sample - loss: 5.1490\n",
      "Epoch 21/100\n",
      "15462/15462 [==============================] - 5s 349us/sample - loss: 5.0649\n",
      "Epoch 22/100\n",
      "15462/15462 [==============================] - 5s 347us/sample - loss: 4.9809\n",
      "Epoch 23/100\n",
      "15462/15462 [==============================] - 5s 339us/sample - loss: 4.8957\n",
      "Epoch 24/100\n",
      "15462/15462 [==============================] - 5s 339us/sample - loss: 4.8100\n",
      "Epoch 25/100\n",
      "15462/15462 [==============================] - ETA: 0s - loss: 4.735 - 5s 339us/sample - loss: 4.7355\n",
      "Epoch 26/100\n",
      "15462/15462 [==============================] - 5s 338us/sample - loss: 4.6521\n",
      "Epoch 27/100\n",
      "15462/15462 [==============================] - 5s 338us/sample - loss: 4.5552s - loss: 4.553\n",
      "Epoch 28/100\n",
      "15462/15462 [==============================] - 5s 336us/sample - loss: 4.4838\n",
      "Epoch 29/100\n",
      "15462/15462 [==============================] - 5s 337us/sample - loss: 4.3852\n",
      "Epoch 30/100\n",
      "15462/15462 [==============================] - 5s 339us/sample - loss: 4.3080\n",
      "Epoch 31/100\n",
      "15462/15462 [==============================] - 5s 337us/sample - loss: 4.2235s - loss:\n",
      "Epoch 32/100\n",
      "15462/15462 [==============================] - 5s 338us/sample - loss: 4.1359\n",
      "Epoch 33/100\n",
      "15462/15462 [==============================] - 5s 336us/sample - loss: 4.0579\n",
      "Epoch 34/100\n",
      "15462/15462 [==============================] - 5s 340us/sample - loss: 3.9672\n",
      "Epoch 35/100\n",
      "15462/15462 [==============================] - 5s 340us/sample - loss: 3.8951\n",
      "Epoch 36/100\n",
      "15462/15462 [==============================] - 5s 346us/sample - loss: 3.8106\n",
      "Epoch 37/100\n",
      "15462/15462 [==============================] - 5s 335us/sample - loss: 3.7390\n",
      "Epoch 38/100\n",
      "15462/15462 [==============================] - 5s 339us/sample - loss: 3.6610\n",
      "Epoch 39/100\n",
      "15462/15462 [==============================] - 5s 337us/sample - loss: 3.5800\n",
      "Epoch 40/100\n",
      "15462/15462 [==============================] - 5s 336us/sample - loss: 3.5004\n",
      "Epoch 41/100\n",
      "15462/15462 [==============================] - 5s 349us/sample - loss: 3.4398\n",
      "Epoch 42/100\n",
      "15462/15462 [==============================] - 6s 360us/sample - loss: 3.3815\n",
      "Epoch 43/100\n",
      "15462/15462 [==============================] - 5s 347us/sample - loss: 3.3098\n",
      "Epoch 44/100\n",
      "15462/15462 [==============================] - 5s 348us/sample - loss: 3.2299\n",
      "Epoch 45/100\n",
      "15462/15462 [==============================] - 5s 346us/sample - loss: 3.1883\n",
      "Epoch 46/100\n",
      "15462/15462 [==============================] - 5s 347us/sample - loss: 3.1149\n",
      "Epoch 47/100\n",
      "15462/15462 [==============================] - 5s 355us/sample - loss: 3.0585\n",
      "Epoch 48/100\n",
      "15462/15462 [==============================] - 5s 344us/sample - loss: 2.9809\n",
      "Epoch 49/100\n",
      "15462/15462 [==============================] - 5s 350us/sample - loss: 2.9366\n",
      "Epoch 50/100\n",
      "15462/15462 [==============================] - 5s 349us/sample - loss: 2.8767\n",
      "Epoch 51/100\n",
      "15462/15462 [==============================] - 5s 346us/sample - loss: 2.8166\n",
      "Epoch 52/100\n",
      "15462/15462 [==============================] - 5s 347us/sample - loss: 2.7716\n",
      "Epoch 53/100\n",
      "15462/15462 [==============================] - 6s 364us/sample - loss: 2.7282s - loss:\n",
      "Epoch 54/100\n",
      "15462/15462 [==============================] - 5s 352us/sample - loss: 2.6790\n",
      "Epoch 55/100\n",
      "15462/15462 [==============================] - 6s 363us/sample - loss: 2.6260\n",
      "Epoch 56/100\n",
      "15462/15462 [==============================] - 6s 401us/sample - loss: 2.5694\n",
      "Epoch 57/100\n",
      "15462/15462 [==============================] - 6s 410us/sample - loss: 2.5244\n",
      "Epoch 58/100\n",
      "15462/15462 [==============================] - 6s 370us/sample - loss: 2.4779\n",
      "Epoch 59/100\n",
      "15462/15462 [==============================] - 6s 377us/sample - loss: 2.4359\n",
      "Epoch 60/100\n",
      "15462/15462 [==============================] - 6s 367us/sample - loss: 2.3906\n",
      "Epoch 61/100\n",
      "15462/15462 [==============================] - 5s 355us/sample - loss: 2.3766\n",
      "Epoch 62/100\n",
      "15462/15462 [==============================] - 6s 357us/sample - loss: 2.3068\n",
      "Epoch 63/100\n",
      "15462/15462 [==============================] - 5s 351us/sample - loss: 2.2650\n",
      "Epoch 64/100\n",
      "15462/15462 [==============================] - 6s 361us/sample - loss: 2.2374\n",
      "Epoch 65/100\n",
      "15462/15462 [==============================] - 5s 346us/sample - loss: 2.2080\n",
      "Epoch 66/100\n",
      "15462/15462 [==============================] - 5s 351us/sample - loss: 2.1805\n",
      "Epoch 67/100\n",
      "15462/15462 [==============================] - 5s 351us/sample - loss: 2.1489\n",
      "Epoch 68/100\n",
      "15462/15462 [==============================] - 5s 354us/sample - loss: 2.1021\n",
      "Epoch 69/100\n",
      "15462/15462 [==============================] - 5s 347us/sample - loss: 2.0587\n",
      "Epoch 70/100\n",
      "15462/15462 [==============================] - 5s 351us/sample - loss: 2.0472\n",
      "Epoch 71/100\n",
      "15462/15462 [==============================] - 5s 350us/sample - loss: 2.0086\n",
      "Epoch 72/100\n",
      "15462/15462 [==============================] - 5s 347us/sample - loss: 1.9813\n",
      "Epoch 73/100\n",
      "15462/15462 [==============================] - 5s 346us/sample - loss: 1.9888\n",
      "Epoch 74/100\n",
      "15462/15462 [==============================] - 5s 350us/sample - loss: 1.9161\n",
      "Epoch 75/100\n",
      "15462/15462 [==============================] - 6s 361us/sample - loss: 1.8992\n",
      "Epoch 76/100\n",
      "15462/15462 [==============================] - 6s 356us/sample - loss: 1.8694\n",
      "Epoch 77/100\n",
      "15462/15462 [==============================] - 5s 346us/sample - loss: 1.8349\n",
      "Epoch 78/100\n",
      "15462/15462 [==============================] - 6s 359us/sample - loss: 1.8188\n",
      "Epoch 79/100\n",
      "15462/15462 [==============================] - 6s 361us/sample - loss: 1.8050\n",
      "Epoch 80/100\n",
      "15462/15462 [==============================] - 6s 376us/sample - loss: 1.7684\n",
      "Epoch 81/100\n",
      "15462/15462 [==============================] - 6s 366us/sample - loss: 1.7535\n",
      "Epoch 82/100\n",
      "15462/15462 [==============================] - 6s 366us/sample - loss: 1.7458\n",
      "Epoch 83/100\n",
      "15462/15462 [==============================] - 6s 370us/sample - loss: 1.7162\n",
      "Epoch 84/100\n",
      "15462/15462 [==============================] - 6s 358us/sample - loss: 1.6938\n",
      "Epoch 85/100\n",
      "15462/15462 [==============================] - 5s 353us/sample - loss: 1.6650\n",
      "Epoch 86/100\n",
      "15462/15462 [==============================] - 6s 369us/sample - loss: 1.6514\n",
      "Epoch 87/100\n",
      "15462/15462 [==============================] - 6s 370us/sample - loss: 1.6249\n",
      "Epoch 88/100\n",
      "15462/15462 [==============================] - 6s 357us/sample - loss: 1.6180\n",
      "Epoch 89/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15462/15462 [==============================] - 5s 355us/sample - loss: 1.5981\n",
      "Epoch 90/100\n",
      "15462/15462 [==============================] - 5s 351us/sample - loss: 1.5635\n",
      "Epoch 91/100\n",
      "15462/15462 [==============================] - 6s 356us/sample - loss: 1.5602\n",
      "Epoch 92/100\n",
      "15462/15462 [==============================] - 5s 353us/sample - loss: 1.5618\n",
      "Epoch 93/100\n",
      "15462/15462 [==============================] - 5s 354us/sample - loss: 1.5344\n",
      "Epoch 94/100\n",
      "15462/15462 [==============================] - 6s 361us/sample - loss: 1.5263\n",
      "Epoch 95/100\n",
      "15462/15462 [==============================] - 6s 361us/sample - loss: 1.5045\n",
      "Epoch 96/100\n",
      "15462/15462 [==============================] - 5s 352us/sample - loss: 1.4968\n",
      "Epoch 97/100\n",
      "15462/15462 [==============================] - 6s 359us/sample - loss: 1.4728\n",
      "Epoch 98/100\n",
      "15462/15462 [==============================] - 6s 361us/sample - loss: 1.4665\n",
      "Epoch 99/100\n",
      "15462/15462 [==============================] - 6s 360us/sample - loss: 1.4452\n",
      "Epoch 100/100\n",
      "15462/15462 [==============================] - 6s 364us/sample - loss: 1.4298\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "hist = model.fit(\n",
    "    predictors, labels,\n",
    "    epochs=100,\n",
    "    verbose=1,\n",
    "    callbacks=[reduce_rl, early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Show training graphs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_graphs(history, metrics):\n",
    "    plt.figure()\n",
    "    for metric in metrics:\n",
    "        plt.plot(history.history[metric], label=metric)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_graphs(hist, [\"loss\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto-generate Poems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I look into your eyes, and I can see down from clouds for love's well might one brain and skill can sit more none so much my dear friend ' grow part light rage are bide dead far new woe new day be allay'd kind ill grow still so grow than ill now made too near of care or kind needing friend ' so 'will gone new one erred hid do behold grow bright days bright cold new fired night had cold did hush the wind come crown'd the pebbled night as men had hush on every truth on to you deem'd sit tell now he forgot be grow\n"
     ]
    }
   ],
   "source": [
    "seed_text = \"I look into your eyes, and I can see\"\n",
    "next_words = 100\n",
    "\n",
    "# index to word dictionary\n",
    "reverse_word_index = dict([(value, key) for (key, value) in tokenizer.word_index.items()])\n",
    "\n",
    "for _ in range(next_words):\n",
    "    # generate sequence\n",
    "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "    # padding\n",
    "    token_list = pad_sequences([token_list], maxlen=max_len-1, padding=PAD_TYPE)\n",
    "    # predict the next word\n",
    "    predicted = model.predict(token_list)\n",
    "    predicted = int(np.argmax(predicted, axis=1).squeeze())\n",
    "    # index to word\n",
    "    output_word = reverse_word_index[predicted]\n",
    "    # add to the seed text\n",
    "    seed_text += \" \" + output_word\n",
    "\n",
    "print(seed_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('tf2-plus': conda)",
   "language": "python",
   "name": "python37764bittf2pluscondaab7d8de0804d40f7b6e27d871405ec4e"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}