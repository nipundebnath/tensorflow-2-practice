{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing\n",
    "- Gives words a unique value.\n",
    "\n",
    "**`tf.keras.preprocessing.text.Tokenizer()`**\n",
    "- By default, removes all punctuations.\n",
    "- Converts all words to lowercase.\n",
    "- [Tensorflow doc](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Transform words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "{'i': 1, 'love': 2, 'my': 3, 'dog': 4, 'cat': 5}\n"
    }
   ],
   "source": [
    "sentences = [\n",
    "    'I love my dog',\n",
    "    'i, love my cat',\n",
    "]\n",
    "\n",
    "# initiate a tokenizer\n",
    "tokenizer = Tokenizer(num_words=100)\n",
    "# update internal vocabulary based on the given list of texts\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "\n",
    "# get the tokens\n",
    "word_index = tokenizer.word_index\n",
    "print(word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: - Calling `fit_on_texts()` on new texts will appned new words to the old vocabulary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'love': 1, 'i': 2, 'my': 3, 'dog': 4, 'cat': 5, 'you': 6}"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "new_sentences = [\n",
    "    'You love dog!'\n",
    "]\n",
    "\n",
    "# update internal vocabulary based on the given list of texts\n",
    "tokenizer.fit_on_texts(new_sentences)\n",
    "\n",
    "# see the new tokens\n",
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Transform sequence of words (a.k.a. text or sentence)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[2, 1, 3, 4], [2, 1, 3, 5]]\n"
    }
   ],
   "source": [
    "# transform each sentence to a sequence of integers\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "print(sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: - Unknown words are ignored!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[[4]]"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "# a sentence with unfamiliar words\n",
    "tokenizer.texts_to_sequences(['She loves dog!'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**:- Use the `oov_token` parameter in `Tokenizer()` to tokenize out-of-vocabulary words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "{'<OOV>': 1, 'love': 2, 'i': 3, 'my': 4, 'dog': 5, 'cat': 6, 'you': 7}\n"
    }
   ],
   "source": [
    "# initiate a tokenizer\n",
    "tokenizer = Tokenizer(num_words=100, oov_token='<OOV>')\n",
    "\n",
    "# update internal vocabulary based on the given list of texts\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "tokenizer.fit_on_texts(new_sentences)\n",
    "\n",
    "# get the tokens\n",
    "word_index = tokenizer.word_index\n",
    "print(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[[1, 1, 5]]"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "# a sentence with unfamiliar words\n",
    "tokenizer.texts_to_sequences(['She loves dog!'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding\n",
    "\n",
    "\n",
    "**`tf.keras.preprocessing.sequence.pad_sequences()`**\n",
    "- Pad sequences by adding a value (param: `value`) at the beginning (`padding='pre'`) or ending (`padding='post'`).\n",
    "- Makes all sequences to equal length (param: `maxlen`). \n",
    "- If `maxlen` is not provided, sequences will be padded to the length of the longest individual sequence.  \n",
    "- If a sequence is longer than the `maxlen`, truncates the beginning (`truncating='pre'`) or ending (`truncating='post'`).\n",
    "- [Tensorflow doc](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[3, 2, 4, 5], [3, 2, 4, 6]]\n\n[[3 2 4 5 0 0]\n [3 2 4 6 0 0]]\n"
    }
   ],
   "source": [
    "# get sequences\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "print(sequences)\n",
    "print()\n",
    "\n",
    "# padding\n",
    "padded = pad_sequences(sequences, padding='post', maxlen=6)\n",
    "print(padded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **EXAMPLE**\n",
    "**Sarcasm Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "DATA_URL = 'https://storage.googleapis.com/laurencemoroney-blog.appspot.com/sarcasm.json'\n",
    "PATH_OUT = '../.tmp/sarcasm.json'\n",
    "\n",
    "# download the dataset\n",
    "os.system(f\"\"\"wget --no-check-certificate {DATA_URL} -O {PATH_OUT}\"\"\")\n",
    "\n",
    "# read the file\n",
    "with open(PATH_OUT, 'r') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'article_link': 'https://www.huffingtonpost.com/entry/versace-black-code_us_5861fbefe4b0de3a08f600d5',\n 'headline': \"former versace store clerk sues over secret 'black code' for minority shoppers\",\n 'is_sarcastic': 0}"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "# see one example\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "\"former versace store clerk sues over secret 'black code' for minority shoppers\""
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "# extract all headlines\n",
    "headlines = []\n",
    "for item in data:\n",
    "    headlines.append(item['headline'])\n",
    "\n",
    "# see one example\n",
    "headlines[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "# words in the vocabulary: 29657\n"
    }
   ],
   "source": [
    "# initiate a tokenizer\n",
    "tokenizer = Tokenizer(oov_token='<OOV>')\n",
    "\n",
    "# update the vocabulary\n",
    "tokenizer.fit_on_texts(headlines)\n",
    "\n",
    "print('# words in the vocabulary:', len(tokenizer.word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "padded sequences:\n[[  308 15115   679 ...     0     0     0]\n [    4  8435  3338 ...     0     0     0]\n [  145   838     2 ...     0     0     0]\n ...\n [10735     9    68 ...     0     0     0]\n [ 1541   392  4164 ...     0     0     0]\n [29656  1647     6 ...     0     0     0]]\nshape (26709, 40)\n"
    }
   ],
   "source": [
    "# get sequences\n",
    "sequences = tokenizer.texts_to_sequences(headlines)\n",
    "\n",
    "# padding\n",
    "padded = pad_sequences(sequences, padding='post')\n",
    "\n",
    "print('padded sequences:')\n",
    "print(padded)\n",
    "print('shape', padded.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-tokenized Dataset\n",
    "\n",
    "**IMDB movie review dataset** ([tfds webpage](https://www.tensorflow.org/datasets/catalog/imdb_reviews)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get the dataset\n",
    "imdb, info = tfds.load(\n",
    "    'imdb_reviews/subwords8k', with_info=True, as_supervised=True, shuffle_files=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the tokenizer\n",
    "tokenizer = info.features['text'].encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Tokenized string is [12, 174, 7, 163, 323, 3, 2939, 9, 74, 5038, 7961, 7, 1843, 27, 2296, 7962]\nThe original string: I love to watch movies. IMDb is good source to check movie rating!\n"
    }
   ],
   "source": [
    "# check one string\n",
    "sample_string = 'I love to watch movies. IMDb is a good source to check movie rating!'\n",
    "\n",
    "tokenized_string = tokenizer.encode(sample_string)\n",
    "print ('Tokenized string is {}'.format(tokenized_string))\n",
    "\n",
    "original_string = tokenizer.decode(tokenized_string)\n",
    "print ('The original string: {}'.format(original_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37764bittf2pluscondaab7d8de0804d40f7b6e27d871405ec4e",
   "display_name": "Python 3.7.7 64-bit ('tf2-plus': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}